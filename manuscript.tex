% ********************************************
%
%   Dear Authors,
% 
%   Please, read the following comments in Preset settings and adjust the settings according to your needs.
%   Please, feel free to add more packages or macros if needed.
%   Details of the pre-defined packages, symbols and macros can be found in the submodule "template". (See PDF files.)
%   If you finished the adjustments, you can remove the comments for a clean document.
% 
%   Any questions or contributions are welcome.
%   Please, contact to the author.
%   Enjoy your writing.
%
%                               Myeongseok Ryu
%  	    				dding_98@gm.gist.ac.kr
%                                  09.Feb.2025
%
% ********************************************

% ********************************************
% Revision History
%   - 01.Jun.2025: Conference template created.
% ********************************************

% ********************************************
% This project is highly ispired by the work of LMRES Lab, Hochschule München.
% Thank you very much my dear friend, Niklas Monzen, for your kind support.
% ********************************************

% ============================================
%         Preset 1. Document Type
% ============================================
\documentclass[conference]{IEEEtran}

% THIS IS NEEDED FOR FINAL SUBMISSION
\IEEEoverridecommandlockouts 
% This command is only needed if you want to use the \thanks commands  
%\overrideIEEEmargins 
% Needed to meet printer requirements.

% ============================================
%         Preset 2. Local Path
% ============================================
% When you create "figures" or "movies" directories in the "src" (source code) directory, the file name including path can be long.
% To avoid this, and to make it easier to adjust, you can define the alias for the path.
% The examples are provided below.

% \newcommand*{\FIGURESPATH}{./figures}
% \newcommand*{\SIMFIGURESPATH}{./src/script_simulation/figures}
% \newcommand*{\SLXFIGURESPATH}{./src/simulink_simulation/figures}
% \newcommand*{\MOVIESPATH}{./movies}

% ============================================
%         Preset 3. Pre-defined Settings
% ============================================
% PLEASE DO NOT ADD OR REMOVE PACKAGES IN THE SUBMODULE LOCALLY!
% CONTACT THE AUTHOR FOR ADJUSTMENTS.
%
% The packages are pre-defined in the submodule "Template".
% If you need more packages, please add them after using pre-defined packages.

\def\cmt{true} % true for comments, false for no comments
\def\pub{false} % true for publication, false for draft
\newcommand*{\template}{template} 
\input{\template/preamble/preamble_conf.tex}

% ============================================
%     Preset 4. Additional Corrections
% ============================================
% correct bad hyphenation here
% \hyphenation{op-tical net-works semi-conduc-tor}
% \pagestyle{empty}

%--- Define custom math operators and notation ---%
\newcommand{\vectheta}{\hat{\theta}}  % -> \estwth [estimated weight (theta)]
\newcommand{\grad}{\nabla}            % -> \nabla (I think nabla is enough)
\newcommand{\btheta}{\bar{\theta}}  
\newcommand{\bOmega}{\bar{\Omega}}
\newcommand{\bB}{\bar{B}}
\newcommand{\sat}{\mathbf{sat}}     % -> \mysat 
\newcommand{\matPhi}{\hat{\Phi}}
\newcommand{\PhiPrime}{\hat{\Phi}'}
\newcommand{\PhiStar}{\Phi^*}
% \newcommand{\norm}[1]{\|#1\|}
\DeclareMathOperator*{\diag}{diag}  % -> \mydiag 
\DeclareMathOperator*{\vect}{vec}   % -> \mysat


\begin{document}

% ============================================
%            TITLE and AUTHORS
% ============================================
\title{
    CVL-based Constrained Optimization Neuro-Adaptive Control (CVL-CONAC) for Uncertain Control-Affine Systems
    % {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured for       https://ieeexplore.ieee.org  and
    % should not be used}
    \thanks{
        This work was supported by my 10 fingers and Macbook Air M1.
    }
} 

\author{
    \IEEEauthorblockN{1\textsuperscript{st} Myeongseok Ryu}
    \IEEEauthorblockA{\textit{Cho Chun Shik Graduate School of Mobility} \\
    \textit{Korean Advanced Institute of Science and Technology (KAIST)}\\
    Daejeon, Republic of Korea \\
    dding\_98@kaist.ac.kr}
\and
    \IEEEauthorblockN{2\textsuperscript{nd} Niklas Monzen}
    \IEEEauthorblockA{\textit{Laboratory for Mechatronic and Renewable Energy Systems} \\
    \textit{Hochschule München (HM) University of Applied Sciences}\\
    Munich, Germany \\
    niklas.monzen@hm.edu}
}

\maketitle 
\thispagestyle{empty}

% ============================================
%         ABSTRACT and KEYWORDS
% ============================================
\begin{abstract}
    This study proposes a Convolutional Layer (CVL)-based Constrained Optimization Neuro-Adaptive Controller (CVL-CONAC) for uncertain control-affine nonlinear systems. 
    The controller approximates the ideal stabilizing law using a hybrid CVL-FCL architecture that processes historical state data, ensuring a robust representation of system dynamics. 
    The adaptation law is derived by formulating the control problem as a constrained minimization problem of the tracking error, subject to weight norm constraints and control input norm constraints. 
    Stability analysis, based on the Lyapunov approach, rigorously proves that the tracking error and all network weight vectors remain Uniformly Ultimately Bounded(UUB), thereby ensuring stable and constrained online adaptation.
\end{abstract}

\begin{IEEEkeywords}
    Neuro-adaptive control, Constrained optimization, Convolutional Layers (CVL),
Lyapunov approach, Asymptotic convergence.
\end{IEEEkeywords}

% ============================================
%         Notation
% ============================================
\section*{Notation}
In this study, the following notation is used: $\mathbb{R}^{n}$ denotes the
$n$-dimensional Euclidean space. $\mathbf{x} = [x_i]$ denotes a vector, and
$\mathbf{A} = [a_{ij}]$ denotes a matrix. $\mathbf{I}_n$ is the $n \times n$
identity matrix. $\vect(\mathbf{A})$ denotes the vectorization of $\mathbf{A}$.
A matrix $\mathbf{P} \succ 0$ ($\mathbf{P} \succeq 0$) is positive definite
(positive semidefinite). $\lambda_{\min}(\mathbf{A})$ denotes the minimum
eigenvalue of $\mathbf{A}$.

%----------------------------------------------------------------%
%--- I. INTRODUCTION --------------------------------------------%
%----------------------------------------------------------------%
\section{Introduction}

\subsection{Motivation and Background}

dummy citation \cite{Zaccarian:2001aa}

Neuro-Adaptive Control (NAC) has emerged as a crucial tool for controlling
systems with unknown dynamics, leveraging the universal approximation
capabilities of Neural Networks (NNs)~[??]. While traditional Lyapunov-based
NACs primarily focus on ensuring the boundedness of the tracking error and
weight estimation error~[??], two key challenges persist in practical
implementation: managing the potential divergence of NN weights and satisfying
physical input constraints imposed by actuators[??].

The Constrained Optimization-Based Neuro-Adaptive Control (CONAC) framework
addresses these limitations by embedding both weight and input constraints
directly into the adaptation process via a unified constrained optimization
problem[??]. This framework solves the dual optimization problem by updating
both the weights and the Lagrange multipliers, guaranteeing satisfaction of the
Karush-Kuhn-Tucker (KKT) conditions at steady state[??].

This paper extends the CONAC methodology to incorporate the feature-capturing
strength of a Convolutional Layer (CVL) architecture. CVLs are
particularly well-suited for processing time-stacked historical sensor data,
represented as a 2D input matrix (Figure \ref{fig:architecture}), allowing the
controller to capture spatio-temporal features essential for approximating
complex, time-dependent dynamics[??].

\subsection{Main Contributions}

The main contributions of this study are threefold:
\begin{itemize}
    \item Adapting the CONAC formulation specifically for a CVL-FCL
    architecture;
    \item Applying the method to a generic control-affine system;
    \item Rigorously proving the UUB of the tracking error and all network
    weights under the derived adaptation law.
\end{itemize}

%----------------------------------------------------------------%
%--- II. PROBLEM FORMULATION & CONTROL LAW ---------------------%
%----------------------------------------------------------------%
\section{Problem Formulation and Control Law}

\subsection{System Dynamics}

We consider an uncertain generic control-affine nonlinear
system, where the control gain matrix $g(x)$ is assumed to be square and
positive definite ($n=m$ and $g(x) \succ 0$):
\begin{equation} \label{eq:sys_full}
    \ddtt\mv{x} 
    = 
    \mv{f}(\mv{x}) 
    + 
    \mv{g}(\mv{x})\mysat(\mv{u})
\end{equation}
where $x \in \mathbb{R}^n$ is the state vector and $u \in \mathbb{R}^n$ is the
control input.

The system is reformulated using a Hurwitz designer matrix $A_c \in \mathbb{R}^{n \times n}$:
\begin{equation}
\label{eq:sys_affine}
\dot{x} = A_c x + f_c(x) + g(x)\,\mathbf{sat}(u)
\end{equation}
where $f_c(x) = f(x) - A_c x$.

\subsection{Ideal Control Law and Error Dynamics}

The objective in defining the Ideal Control Law $u^*$ is to find a control
input that forces the tracking error $\mathbf{e} = x - x_{ref}$ dynamics into
the desired stable linear form $\dot{e} = A_c e$. This condition is satisfied
if:
\begin{equation*}
u^* = -g^{-1}(x) \big (  A_c x_{ref} + f_c(x) - \dot{x}_{ref}   \big)
\end{equation*}

However, since $f_c(x)$ and $g(x)$ are unknown, the ideal controller(i.e. $u^*$) can't be
 realized. Hence, $u^*$ is approximated by the CVL-CONAC output $\hat{\Phi}(X,
 \vectheta)$. We assume the existence of an ideal network output $\Phi^*$ such
 that $u^* = -\Phi^* - \epsilon$, where $\epsilon$ is the bounded approximation
 error.

The CVL-CONAC control input is defined as an end-to-end policy:
\begin{equation}
\label{eq:control_law}
u = -\hat{\Phi}(X, \vectheta)
\end{equation}
Substituting this into the system dynamics, the tracking error dynamics become:
\begin{equation}
\label{eq:error_dynamics}
\dot{e} = A_c e + g(x) \big( \Phi^* + \epsilon + \mathbf{sat}(-\hat{\Phi}) \big)
\end{equation}
This formulation is used for stability analysis.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth, height=0.2\textheight, keepaspectratio]{figures/CVLCONACArch.png}
    \caption{Proposed CVL-CONAC architecture.}
    \label{fig:architecture}
\end{figure}

%----------------------------------------------------------------%
%--- III. CVL-CONAC ARCHITECTURE & OPTIMIZATION ----------------%
%----------------------------------------------------------------%
\section{System Details}

\subsection{CVL-CONAC Architecture}

The network $\hat{\Phi}$ is designed as a hybrid architecture suitable for
capturing spatio-temporal dynamics from historical data[??].

\subsubsection{Convolutional Layers (CLs)}
The CVLs are represented recursively as:
\begin{equation}
\label{eq:cvl_output}
\Phi_{j_c}^{C} = \begin{cases} O(\phi_{j_c}^{C}(\Phi_{j_c-1}^{C}),\Omega_{j_c},B_{j_c}), & j_c \in [1,\dots,k_c] \\ O(\phi_{0}^{C}(X),\Omega_{0},B_{0}), & j_c = 0 \end{cases}
\end{equation}
where $X \in \mathbb{R}^{n_0 \times m_0}$ denotes the network input matrix, $O:
\mathbb{R}^{n_{j_c} \times m_{j_c}} \times \mathbb{R}^{p_{j_c} \times m_{j_c}
\times q_{j_c}} \times \mathbb{R}^{q_{j_c}} \to \mathbb{R}^{n_{j_c+1} \times
m_{j_c+1}}$ denotes the CNN operator (see Appendix A), and $\phi_{j_c}^{C}:
\mathbb{R}^{n_{j_c} \times m_{j_c}} \to \mathbb{R}^{n_{j_c} \times m_{j_c}}$
denotes the matrix activation function (i.e.,
$\phi_{j_c}^{C}(\Phi_{j_c-1}^{C})_{(i,j)} =
\sigma_{j_c}(\Phi_{j_c-1}^{C})_{(i,j)}$) for some activation function
$\sigma_{j_c}: \mathbb{R} \to \mathbb{R}$. The first activation function
$\phi_{0}^{C}$ should be a bounded nonlinear function to ensure that the input
to the first CNN operator is bounded. In this study, $\alpha_1 \tanh(\cdot)$ is
selected with $\alpha_1 \in \mathbb{R}_{>0}$. The filter set $\Omega_{j_c}$
contains $q_{j_c}$ filters $W_{j_c}^{(i)} \in \mathbb{R}^{p_{j_c} \times
m_{j_c}}$, $\forall i \in [1,\dots,q_{j_c}]$, and is represented as
$\Omega_{j_c} = \{W_{j_c}^{(1)},\dots,W_{j_c}^{(q_{j_c})}\}$, where superscript
$i$ denotes the filter index. The bias vector $B_{j_c}$ consists of
$q_{j_c}$ biases.

The weights for the $j_c$-th CVL layer are collectively represented as
$\hat{\theta}_{Cj_c} \triangleq [\text{vec}(\Omega_{j_c})^T, B_{j_c}^T]^T$.

\subsubsection{Fully-Connected Layers (FCLs)}
The output matrix of the CVLs is input to the concatenate layer
$C(\Phi_{k_c}^{C}) = [\text{vec}(\Phi_{k_c}^{C^{\top}})^{\top}, 1]^{\top}$
before the input layer of FCLs for compatibility between the CVLs and FCLs.
The FCLs are represented recursively as:
\begin{equation}
\label{eq:fcl_output}
\Phi_{j_f}^{F} = \begin{cases} V_{j_f}^{\top}\phi_{j_f}^{F}(\Phi_{j_f-1}^{F}), & j_f \in [1,\dots,k_f] \\ V_{0}^{\top}C(\Phi_{k_c}^{C}), & j_f = 0 \end{cases}
\end{equation}
where $V_{j_f} \in \mathbb{R}^{l_{j_f}+1 \times l_{j_f+1}}$ denotes the weight
matrix, $\phi_{j_f}^{F}: \mathbb{R}^{l_{j_f}} \to \mathbb{R}^{l_{j_f}+1}$
denotes the vector activation function defined by
$\phi_{j_f}^{F}(\Phi_{j_f-1}^{F}) \triangleq
[\sigma_{j_f}(\Phi_{j_f-1}^{F})^\top, 1]^\top$ for $j_f \in [1,\dots,k_f-1]$,
for some nonlinear activation functions $\sigma_{j_f}: \mathbb{R} \to
\mathbb{R}$ such as $\tanh(\cdot)$. Note that $C(\Phi_{k_c}^{C})$ and
$\Phi_{j_f}^{F}$ are augmented by $1$ to consider the bias of the preceding
input layer as a weight in the weight matrix. The output of the FCLs is the
final output of the CNN architecture and is represented as $\hat{\Phi} \equiv
\Phi_{k_f}^{F}$.

The FCL weights for the $j_f$-th layer are $\hat{\theta}_{Fj_f} \triangleq \text{vec}(V_{j_f})$.
%----------------------------------------------------------------%
%--- IV. ADAPTATION LAWS ----------------------------------------%
%----------------------------------------------------------------%
\section{Adaptation Laws}

\subsection{Optimization Problem}
The adaptation law is derived by solving the following constrained minimization problem. We first define the complete set of trainable weights to be constrained:
\begin{equation}
\label{eq:total_index_set}
\mathcal{I} = \{C0, C1, \dots, Ck_c\} \cup \{F0, F1, \dots, Fk_f\}
\end{equation}
The constrained minimization problem is:
\begin{equation}
\label{eq:optimization}
\begin{aligned}
\min_{\vectheta} \quad & \mathcal{J}(e, \vectheta) = \frac{1}{2}e^{\top}e \\
\text{s.t.} \quad & c_i(\vectheta) = \frac{1}{2} \left( \|\hat{\theta}_i\|^2 - \bar{\theta}_i^2 \right) \le 0, \quad \forall i \in \mathcal{I} \\
& c_{\Phi}(\vectheta) = \frac{1}{2} \left( \|\hat{\Phi}\|^2 - \bar{u}^2 \right) \le 0
\end{aligned}
\end{equation}
where $c_i$ are the weight norm constraints (ensuring boundedness) and
$c_{\Phi}$ is the output norm constraint (ensuring input saturation limits
$\bar{u}$ are respected).
The Lagrangian function is made generic over the index set $\mathcal{I}$:
\begin{equation}
\label{eq:lagrangian}
\mathcal{L}(\vectheta, \boldsymbol{\lambda}) = \frac{1}{2}e^{\top}e + \sum_{i \in \mathcal{I}} \lambda_i c_i(\vectheta) + \lambda_{\Phi} c_{\Phi}(\vectheta)
\end{equation}
where $\lambda_i, \lambda_{\Phi} \ge 0$ are the Lagrange multipliers.


\subsection{Primal Update}
The weight update for each block $\hat{\theta}_i$ is proportional to the
negative gradient of the Lagrangian: $\dot{\hat{\theta}}_i = -\alpha
\nabla_{\hat{\theta}_i} \mathcal{L}$.

\subsubsection{Objective Gradient ($\nabla_{\hat{\theta}_i} \mathcal{J}$)} The
objective gradient is approximated using the chain rule and the sensitivity 
approximation $\frac{\partial x}{\partial u} \approx \mathbf{I_n}$:
\begin{equation*}
\nabla_{\hat{\theta}_i} \mathcal{J} = \left(\frac{\partial e}{\partial \hat{\theta}_i}\right)^T e \approx \left(\frac{\partial x}{\partial u}\frac{\partial u}{\partial \hat{\theta}_i}\right)^T e = - \left(\frac{\partial \hat{\Phi}}{\partial \hat{\theta}_i}\right)^T e
\end{equation*}

\subsubsection{Constraint Gradients ($\nabla_{\hat{\theta}_i} c_j$)}
The gradients of the constraints are:
\begin{itemize}
    \item Weight Constraint: $\nabla_{\hat{\theta}_i} c_i = \hat{\theta}_i$
    \item Output Constraint : $\nabla_{\hat{\theta}_i} c_{\Phi} = \left(\frac{\partial \hat{\Phi}}{\partial \hat{\theta}_i}\right)^T \hat{\Phi}$
\end{itemize}

\subsubsection{Final Primal Update Rule}
The resulting weight update rule for block $\hat{\theta}_i$ is:
\begin{equation}
\label{eq:primal_update}
\dot{\hat{\theta}}_i = \alpha \left[ \left(\frac{\partial \hat{\Phi}}{\partial \hat{\theta}_i}\right)^T e - \lambda_i \hat{\theta}_i - \lambda_{\Phi} \left(\frac{\partial \hat{\Phi}}{\partial \hat{\theta}_i}\right)^T \hat{\Phi} \right]
\end{equation}
where $\alpha \in \mathbb{R}_{>0}$ is the learning rate.

\subsection{Dual Update}
The Lagrange multipliers are updated using projected gradient ascent:
\begin{align}
\label{eq:dual_update}
\dot{\lambda}_j &= \beta_j c_j(\vectheta), \quad \forall j \in \{i, \Phi\} \\
\lambda_j &= \max(\lambda_j,0)
\end{align}
where $\beta_j \in \mathbb{R}_{>0}$ is the update rate for the corresponding multiplier[??].





\subsection{Network Jacobians}
The adaptation law requires the block Jacobians $\hat{\Phi}_i' \triangleq
\frac{\partial \hat{\Phi}}{\partial \hat{\theta}_i}$, where $\hat{\Phi} \equiv
\Phi_{k_f}^{F}$.

\subsubsection{FCL Jacobians ($\hat{\theta}_{F j_f}$, $j_f \in [0, \dots, k_f]$)}

The Jacobians of $\hat{\Phi}$ with respect to the weights of FCLs are derived
recursively.

For the input layer weights ($j_f = 0$):
\begin{equation}
\label{eq:jac_f0_generic}
\frac{\partial \hat{\Phi}}{\partial \text{vec}(V_{0})} = \Big(\prod_{l=1}^{k_f} V_{l}^{\top} \phi_{l}^{F'} \Big) (I_{l_0} \otimes C(\Phi_{k_c}^{C}))
\end{equation}

For the inner and output layer weights ($j_f \in [1, \dots, k_f]$):
\begin{equation}
\label{eq:jac_f_j_generic}
\frac{\partial \hat{\Phi}}{\partial \text{vec}(V_{j_f})} = \Big(\prod_{l=j_f+1}^{k_f} V_{l}^{\top} \phi_{l}^{F'} \Big) (I_{l_{j_f}} \otimes \Phi_{j_f-1}^{F^{\top}})
\end{equation}
where $\phi_{j_f}^{F'} \triangleq \frac{\partial \phi_{j_f}^{F}(x)}{\partial x}$
is the Jacobian of the activation function with respect to some vector $x$.

\subsubsection{CVL Jacobians ($\hat{\theta}_{C j_c}$, $j_c \in [0, \dots, k_c]$)}

The Jacobians of $\hat{\Phi}$ with respect to the weights of CVLs (filters
$\Omega_{j_c}$ and biases $B_{j_c}$) are derived using the backpropagation
method, linking the output sensitivity $\frac{\partial \hat{\Phi}}{\partial
\Phi_{j_c}^{C}}$ backward through the convolutional layers.

Let $\Phi_i$ denote the $i$-th output of $\hat{\Phi}$. The Jacobians are:


\begin{IEEEeqnarray}{l}
\frac{\partial \Phi_i}{\partial W_{j_c}^{l_k}} =
\sum_{l_i=1}^{n_{j_c+1}} 
\Big(
\frac{\partial \Phi_i}{\partial \Phi_{j_c}^{C}(l_i,l_k)}
\operatorname{row}(l_b:l_e) (\phi_{j_c}^{C})
\Big), \label{eq:jac_c_omega}
\\
\frac{\partial \Phi_i}{\partial B_{j_c}(l_k)} =
\sum_{l_i=1}^{n_{j_c+1}} 
\Big(
\frac{\partial \Phi_i}{\partial \Phi_{j_c}^{C}(l_i,l_k)}
\cdot 1
\Big), \label{eq:jac_c_bias}
\end{IEEEeqnarray}
with $l_b \triangleq l_i$, $l_e \triangleq l_i+p_{j_c}-1$,  $j_c \in [0, \dots, k_c]$

for all $l_k \in [1, \dots, q_{j_c}]$, where
$\Phi_{j_c}^{C} \triangleq \phi_{j_c}^{C}(\Phi_{j_c-1}^{C})$ for
$j_c \in [1, \dots, k_c]$, $\Phi_{0}^{C} \triangleq \phi_{0}^{C}(X)$, and
$\partial \Phi_i / \partial \Phi_{j_c}^{C}$ denotes the backpropagated gradient
of $\Phi_i$ with respect to $\Phi_{j_c}^{C}$
(obtained using the back-propagation method, detailed in Appendix B).


%----------------------------------------------------------------%
%--- V. STABILITY ANALYSIS (CVL-CONAC) -------------------------%
%----------------------------------------------------------------%
\section{Stability Analysis}
The stability analysis aims to prove that the filtered tracking error $e$ and
all estimated weight vectors $\hat{\theta}$ remain Uniformly Ultimately
Bounded (UUB), provided the weight norm constraints are imposed.

\subsection{Boundedness of $e$}
Let the Lyapunov function for the error be $V_e = \frac{1}{2} e^T P e$, with $P$
satisfying $A_c^T P + P A_c = -Q$ ($Q \succ 0$).

The time derivative $\dot{V}_e$ is derived from the error dynamics (\ref{eq:error_dynamics}):
$$\dot{V}_e = e^T P \left[ A_c e + g(x) \big( \Phi^* + \epsilon +
\mathbf{sat}(-\hat{\Phi}) \big) \right]$$ Using the properties of $P$ and $A_c$,
the derivative is bounded by:
$$\dot{V}_e \le -\frac{1}{2} \lambda_{\min}(Q) \|e\|^2 + \|e\| \|P g(x)\| \left(
\|\Phi^*\| + \|\epsilon\| + \|\mathbf{sat}(-\hat{\Phi})\| \right)$$ Assuming the
perturbations and control bounds are finite ($\Psi = \sup \{\|\Phi^*\|,
\|\epsilon\|,\|\mathbf{sat}(-\hat{\Phi})\|\}  $) and $\|P g(x)\| \le \gamma_e$,
the ultimate bounding set $\Omega_e$ for the tracking error $e$ is:
$$e \in \Omega_e = \left\{ e \mid \|e\| \le \frac{2 \gamma_e
\Psi}{\lambda_{\min}(Q)} \right\}$$ Thus, the tracking error $e$ is
UUB.

\subsection{Boundedness of Output Layer Weights ($\hat{\theta}_{Fj_{kf}}$)} The

\subsubsection{Case 1: Control Input Saturated ($\lambda_{\Phi} \ge 0$)} In this
case, the input constraint is active, and $\lambda_{\Phi} \ge 0$. We analyze the
time derivative of the Lyapunov candidate for the estimated weights
$V_{\hat{\theta}_{Fj_{kf}}} = \frac{1}{2\alpha} \hat{\theta}_{Fj_{kf}}^T
\hat{\theta}_{Fj_{kf}}$.

The derivative $\dot{V}_{\hat{\theta}_{Fj_{kf}}}$ is obtained by substituting the primal update law for $\dot{\hat{\theta}}_{Fj_{kf}}$:
$$
\dot{V}_{\hat{\theta}_{Fj_{kf}}} = \hat{\theta}_{Fj_{kf}}^T (\hat{\Phi}_{Fj_{kf}}')^T e - \lambda_{Fj_{kf}} \|\hat{\theta}_{Fj_{kf}}\|^2 - \lambda_{\Phi} \hat{\theta}_{Fj_{kf}}^T (\hat{\Phi}_{Fj_{kf}}')^T \hat{\Phi}
$$
Since the constraint term is non-positive ($-\lambda_{\Phi} \hat{\theta}_{Fj_{kf}}^T (\hat{\Phi}_{Fj_{kf}}')^T \hat{\Phi} \le 0$), we drop it for worst-case analysis. We use the fact that the Jacobian norm is bounded: $\left\| (\hat{\Phi}_{Fj_{kf}}')^T \right\| \le \gamma_{Fj_{kf}}$. Applying Cauchy-Schwarz:
$$
\dot{V}_{\hat{\theta}_{Fj_{kf}}} \le \gamma_{Fj_{kf}} \|e\| \|\hat{\theta}_{Fj_{kf}}\| - \lambda_{Fj_{kf}} \|\hat{\theta}_{Fj_{kf}}\|^2
$$
Since $\lambda_{Fj_{kf}} > 0$, $\dot{V}_{\hat{\theta}_{Fj_{kf}}}$ is negative outside the compact set where:
\begin{align*}
\hat{\theta}_{Fj_{kf}} \in \Omega_{\hat{\theta}_{Fj_{kf}}} \triangleq \left\{ \hat{\theta}_{Fj_{kf}} \mid \|\hat{\theta}_{Fj_{kf}}\| \le \frac{\gamma_{Fj_{kf}}}{\lambda_{Fj_{kf}}} \|e\| \right\}
\end{align*}
Since $\|e\|$ is UUB, $\hat{\theta}_{Fj_{kf}}$ is also UUB.

\subsubsection{Case 2: Control Input Not Saturated ($\lambda_{\Phi}=0$)}

With $\lambda_{\Phi}=0$ we set $V_3=V_e+V_{\hat\theta_{F_{j_{kf}}}}$ and treat
the two parts separately.

\paragraph{Error Lyapunov derivative.}
Using the error dynamics,
\begin{equation}
\dot V_e = -\tfrac{1}{2} e^\top Q e + e^\top P g(x)\big(\Phi^* + \epsilon - \hat\Phi\big).
\end{equation}
Apply the first-order expansion
\[
\Phi^* - \hat\Phi
= \hat\Phi'_{F_{j_{kf}}}\big(\theta^*_{F_{j_{kf}}}-\hat\theta_{F_{j_{kf}}}\big) + \eta,
\]
where \(\eta\) collects higher-order terms, to obtain
\begin{align}
\dot V_e
&= -\tfrac{1}{2} e^\top Q e
+ e^\top P g(x)\hat\Phi'_{F_{j_{kf}}}\theta^*_{F_{j_{kf}}}
- e^\top P g(x)\hat\Phi'_{F_{j_{kf}}}\hat\theta_{F_{j_{kf}}}
\nonumber\\[-2pt]
&\qquad + e^\top P g(x)(\epsilon+\eta).
\end{align}

\paragraph{Parameter Lyapunov derivative.}
For the parameter part (adaptive law with \(\lambda_{F_{j_{kf}}}\)),
\begin{equation}
\dot V_{\hat\theta_{F_{j_{kf}}}}
= \hat\theta_{F_{j_{kf}}}^\top(\hat\Phi'_{F_{j_{kf}}})^\top e
- \lambda_{F_{j_{kf}}}\|\hat\theta_{F_{j_{kf}}}\|^2.
\end{equation}

\paragraph{Combine and group coupling terms.}
Summing \(\dot V_e+\dot V_{\hat\theta_{F_{j_{kf}}}}\) groups the terms with \(\hat\theta\):
\begin{align}
\dot V_3
&= -\tfrac{1}{2} e^\top Q e
+ e^\top P g(x)\hat\Phi'_{F_{j_{kf}}}\theta^*_{F_{j_{kf}}}
+ e^\top P g(x)(\epsilon+\eta)
\nonumber\\[-2pt]
&\qquad + e^\top\big(\hat\Phi'_{F_{j_{kf}}}-Pg(x)\hat\Phi'_{F_{j_{kf}}}\big)\hat\theta_{F_{j_{kf}}}
- \lambda_{F_{j_{kf}}}\|\hat\theta_{F_{j_{kf}}}\|^2 .
\label{eq:V3_combined}
\end{align}

\paragraph{Assumptions and operator bounds.}
We use
\[
e^\top Q e \ge \lambda_{\min}(Q)\|e\|^2,\quad
\|Pg(x)\|\le\gamma_e,\quad
\|\hat\Phi'_{F_{j_{kf}}}\|\le\gamma_{F_{j_{kf}}},
\]
\[
\|\epsilon+\eta\|\le\rho,\quad
\|\theta^*_{F_{j_{kf}}}\|\le\overline{\theta}_{F_{j_{kf}}}.
\]
And by defining:
\[
M_{F_{j_{kf}}}\triangleq
\hat\Phi'_{F_{j_{kf}}}-Pg(x)\hat\Phi'_{F_{j_{kf}}},
\qquad
\|M_{F_{j_{kf}}}\|\le
\gamma_{\hat\theta}
\triangleq
\gamma_{F_{j_{kf}}}(1+\gamma_e).
\]

\paragraph{Term-by-term bounds.}
\begin{align*}
\big|e^\top Pg(x)\hat\Phi'_{F_{j_{kf}}}\theta^*_{F_{j_{kf}}}\big|
&\le
\gamma_e\gamma_{F_{j_{kf}}}\overline{\theta}_{F_{j_{kf}}}\|e\|,\\
\big|e^\top Pg(x)(\epsilon+\eta)\big|
&\le \gamma_e\rho\|e\|,\\
\big|e^\top M_{F_{j_{kf}}}\hat\theta_{F_{j_{kf}}}\big|
&\le \gamma_{\hat\theta}\|e\|\,\|\hat\theta_{F_{j_{kf}}}\|.
\end{align*}

Apply Young's inequality to the last coupling term with design constant \(\alpha_3>0\):
\[
\gamma_{\hat\theta}\|e\|\,\|\hat\theta\|
\le \frac{1}{2\alpha_3}\|e\|^2
+ \frac{\alpha_3\gamma_{\hat\theta}^2}{2}\|\hat\theta\|^2.
\]

\paragraph{Final Lyapunov bound.}
Collecting the bounds into \eqref{eq:V3_combined} yields
\begin{align}
\dot V_3
\le&\;
\Big(-\tfrac{1}{2}\lambda_{\min}(Q)+\tfrac{1}{2\alpha_3}\Big)\|e\|^2
+ \big(\gamma_e\gamma_{F_{j_{kf}}}\overline{\theta}_{F_{j_{kf}}}
+ \gamma_e\rho\big)\|e\|
\nonumber\\[-2pt]
&\;
+\Big(\tfrac{\alpha_3\gamma_{\hat\theta}^2}{2}
- \lambda_{F_{j_{kf}}}\Big)
\|\hat\theta_{F_{j_{kf}}}\|^2 .
\label{eq:V3_final_bound}
\end{align}

If the parameters are chosen such that:
\(\alpha_3>\tfrac{1}{\lambda_{\min}(Q)}\) and
\(\lambda_{F_{j_{kf}}}>\tfrac{\alpha_3\gamma_{\hat\theta}^2}{2}\). Define

Then:
\[
k_e\triangleq\tfrac{1}{2}\lambda_{\min}(Q)-\tfrac{1}{2\alpha_3}>0,\qquad
k_\theta\triangleq\lambda_{F_{j_{kf}}}-\tfrac{\alpha_3\gamma_{\hat\theta}^2}{2}>0,
\]
and
\[
c\triangleq\gamma_e\big(\gamma_{F_{j_{kf}}}\overline{\theta}_{F_{j_{kf}}}+\rho\big).
\]

From \eqref{eq:V3_final_bound}
\[
\dot V_3 \le
-k_e\|e\|^2
- k_\theta\|\hat\theta_{F_{j_{kf}}}\|^2
+ c\|e\|.
\]

The final bound for the combined Lyapunov function derivative in Case 2 is given by:
\[
\dot V_3 \le -k_e\|e\|^2 - k_\theta\|\hat\theta_{Fj_{kf}}\|^2 + c\|e\|.
\]
Since the design parameters are chosen such that $k_e > 0$ and $k_\theta > 0$,
the derivative $\dot V_3$ is guaranteed to be negative outside a compact set,
confirming that the combined state $(e, \hat{\theta}_{Fj_{kf}})$ is Ultimately
Uniformly Bounded (UUB).

The ultimate bounding sets are therefore given by:
\begin{enumerate}
    \item Ultimate Bounding Set for Tracking Error ($\Omega_e$):
    \begin{align*}
    e \in \Omega_e \triangleq \left\{ e \mid \|e\| \le \frac{c}{k_e} \right\}
    \end{align*}
    \item Ultimate Bounding Set for Weights ($\Omega_{\hat{\theta}}$):
    \begin{align*}
    \hat{\theta}_{Fj_{kf}} \in \Omega_{\hat{\theta}_{Fj_{kf}}} \triangleq \left\{ \hat{\theta}_{Fj_{kf}} \mid \|\hat{\theta}_{Fj_{kf}}\| \le \sqrt{\frac{c}{k_\theta} \|e\|} \right\}
    \end{align*}
\end{enumerate}
Thus, the tracking error $e$ and the output layer weights $\hat{\theta}_{Fj_{kf}}$ are UUB.


\subsection{Boundedness of Inner Layer Weights ($\hat{\theta}_{j}$ for $j$ being
an Inner Layer)}

The boundedness of all inner layers is established recursively, relying
fundamentally on the boundedness of the outermost layer weights.

The Lyapunov derivative for a generic inner block $\hat{\theta}_j$ is bounded
by:
\begin{equation}
\label{eq:inner_lyap}
\dot{V}_{\hat{\theta}_j} \le \gamma_j \|e\| \|\hat{\theta}_j\| + \lambda_{\Phi} \gamma_j \|\hat{\Phi}\| \|\hat{\theta}_j\| - \lambda_j \|\hat{\theta}_j\|^2
\end{equation}
where $\gamma_j \triangleq \|\hat{\Phi}_j'\|$ is the norm of the backpropagated
Jacobian associated with layer $j$.

The stability of $\hat{\theta}_j$ relies on the crucial condition that the
Jacobian norm $\gamma_j = \|\hat{\Phi}_j'\|$ is bounded if and only if all
subsequent weights $\|\hat{\theta}_k\|$ (where $k$ is closer to the output) are
bounded.

\begin{enumerate}
\item Base Case (Output Layer $\hat{\theta}_{F_{j_{kf}}}$):
The output-layer weights $\hat{\theta}_{F_{j_{kf}}}$ were rigorously proven
to be uniformly ultimately bounded (UUB) in the previous section
(Case~1 and Case~2 analysis).

\item Recursive Step (Backward Pass):
We proceed recursively from the output layer towards the input.
\begin{itemize}
\item Fully Connected Layer (FCL) Weights
($\hat{\theta}_{F_0}, \dots$):
Consider the layer immediately preceding the output, $\hat{\theta}_{F_0}$.
The associated Jacobian $\hat{\Phi}'_{F_0}$ depends on
$\hat{\theta}_{F_{j_{kf}}}$ through the activation derivative
$\phi'_{F_{j_{kf}}}$.
Since $\|\hat{\theta}_{F_{j_{kf}}}\|$ is UUB, it follows that
$\|\hat{\Phi}'_{F_0}\|$ is bounded by a constant $\gamma_{F_0}$.
Applying this bound to~(\ref{eq:inner_lyap}) implies that
$\hat{\theta}_{F_0}$ is UUB.

\item Convolutional Layer (CL) Weights
($\hat{\theta}_{C_i}, \dots$):
The Jacobians for the convolutional layers, $\hat{\Phi}'_{C_i}$,
depend on the weights of all subsequent layers (both FCL and CL layers
closer to the output).
Since all subsequent weights are recursively shown to be bounded,
each Jacobian norm $\gamma_{C_i}$ is bounded.
Consequently, applying the bound to~(\ref{eq:inner_lyap}) shows that
all convolutional-layer weights $\hat{\theta}_{C_i}$ are UUB.
\end{itemize}
\end{enumerate}



Thus, the imposition of the weight norm constraints $c_j$ and the resulting non-zero Lagrange multipliers $\lambda_j$ ensures, through a recursive argument stemming from the bounded output layer, that all inner layer weight vectors are UUB.

%----------------------------------------------------------------%
%--- SIMULATIONS----------------------------------------------------% 
%----------------------------------------------------------------%
\section{Simulations}
To validate the proposed CVL-CONAC

%--- Results Placeholder ---%
\begin{figure*}[!t]
  \centering
  \includegraphics{figures/dummy_figure.JPG}
  \caption{Tracking error comparison: CVL-CONAC vs. traditional NAC. (Placeholder for Figure 3)}
  \label{fig:tracking_errors}
\end{figure*}

%----------------------------------------------------------------%
%--- VI. CONCLUSION & FUTURE WORK -------------------------------%
%----------------------------------------------------------------%
\section{Conclusion and Future Work}

The proposed CVL-CONAC successfully adapted the constrained optimization
framework to a hybrid CVL-FCL architecture, providing a stable, constrained
learning solution for uncertain control-affine nonlinear systems. By formulating
the control problem as a constrained minimization problem, the derived
adaptation laws guarantee that both the tracking error $e$ and all network
weights $\hat{\theta}$ (including FCL and CVL layers) are \textbf{Uniformly
Ultimately Bounded}. The constrained formulation rigorously manages weight norms
and ensures the control input adheres to pre-defined physical limits during the
online learning process, a critical requirement for practical implementation.

Future work will involve real-time experimental validation of the CVL-CONAC
architecture and detailed investigation into optimal CVL filter topology
selection.

% ============================================
%                   Appendix
% ============================================
%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
\appendix

% ============================================
%                   Bibliography
% ============================================
\bibliographystyle{IEEEtran}
\bibliography{\template/refs}

\end{document}